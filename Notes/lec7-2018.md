# Source - Races and Parallelism
https://www.youtube.com/watch?v=a_R_DpsENfk

# Notes
Data races: recall what they are and worry about them. Be especially careful when writing to packed data structures (or apparently
not because it might not matter for x86-64 Haswell) because the compiler might implement them, in a higher optimization level, as
a single word.

Recall the 2-iteration x++ race bug (i.e. x++ is not atomic).

Types of races: read/write or write/read and write/write. Not really any races with read/read. They call sections without cross-dependencies
as "independent."

Familiarize yourself with the cilk system: cilk_spawn, cilk_sync (wait i.e. thread join), and cilk_for (basically does what they call
"divide and conquer" on the loop; important to check that your loop iterations do not depend on previous iterations to avoid data races).

Recall the cilk example with the Fibonacci sequence. They like to draw graphs in which nodes are "strands" which are like LLVMIR basic blocks but for calls/spawns: chunks of code without any calls, spawns, returns, or whatever. Edges in the graph are the calls, spawns, etcetera. This computation DAG which they describe unfolds dynamically (i.e. it's not precomputed). It's also processor oblivious, which supposedly means it'll work for any processor. It's not clear whether that means its optimized or not.

Cilk has a nice data race detecter. Supposedly, it gives you very strong guarantees (i.e. any possible race on the given code will 
be found). The programmer can also use regression tests. This is great, because cilk tries to guarantee that your parallel program will behave identically to the sequential version (recall previously).

Common Sense (Amdahl's Law) states that if you have 1/k of your program as serial, then you cannot get more than a k times speedup.

## More Precise Analysis
We analyze a dependency graph of atomic operations.

We define T_i as the time taken by i processors. We count in "work" (which you can think of as "cycles"). T_1 is literally
just the work taken. T_inf is the time taken by infinite parallel processes which is at most the critical path of the dependency
graph of the atomic operations of the program.

We can look at two subgraphs A -> B such that B is dependent on A. Then if we run them in sequence their joint T_1 is obviously the sum of
the respective T_1s, while their joint T_inf is the sum of their respective T_infs. If we run them in parallel, their joint T_1 is the same
as in sequence, but their joint T_inf is the max of their respective T_infs. Through this law, and composition, you can basically look at whole programs' Ts.

Note that for P processors, T_P >= T_1 x P. Obviously, T_P <= T_inf as well.

They define T_1 / T_inf as the parallelism of the program. This is basically the potential speedup.

## Cilk Scale
Cilk has a method to let you analyze the parallelism of a program. It computes both work and span to calculate bounds on parallel performance. Honestly, I'm not sure how they do it: something, something AST I'm guessing. How would you detect runtimes like that?

It seems to be running on actual example problems, so we'll find out though.

## Parallel Quicksort: Example
The parallelism was around logN. That's because the number of levels of quicksort is around logN. The work is NlogN, and the critical path has a length of N (it's a geometrically decreasing series of the partitions because they roughly decrease by half, so you end up with N +
N/2 + N/4 + ... assuming you have enough processors to do ALL the partitions at the same time).

The real cost comes from the partition. In merge sort its from the merge. You want, not just your recursive calls, but also your merge
or your partition to happen in parallel as much as possible.

Merge sort is supposedly even better because it has a span of lg^3N (idk why). Matrix multiplication has lgN span, Strassen has lg^2N,
etc... Just look at the slide.

## Scheduling Theory
There are distributed and centralized schedulers. Cilk's is distributed. We'll look primarily at centralized since they are
a lot simpler.

### Greedy Scheduler
THe goal is to do as much as possible at the current timestep without thinking too much about the future. He gives an example
Of a point at which we have done certain stands (blue) and then want to do the next strands (yellow). A complete span is to run P strands
if we have >= P strands ready and P processors.

Any greedy scheduler is such that T_P <= T_1 / P + T_inf. The number of complete steps where we execute up to P elements must be at most
T_1 / P since that would exhaust our work. When we execute an incomplete step there are not over P elements to pick: thus, there must be a strand that is in the critical path; thus, we must reduce the critical path length by one. Why must this be true? After T_1 / P complete steps and T_inf incomplete steps you must be done. Order? A little hard to visualize for me.

The optimal scheduler is the max of T_1 / P and T_inf (recall work and span laws). Thus any greedy scheduler is within a factor of two of an optimal scheduler.

A greedy scheduler achieves near linear perfect speedup whenever P << T_1 / T_inf. Using the upper bound on T_P note that the previous equation is the same as T_inf << T_1 / P which means that T_P ~= T_1 / P -> T_1 / T_P ~= P. Can you visualize this?

The quantity T_1/(P x T_inf) is called the parallel slackness. The higher this is, the more you can usually parallelize. There is overhead to scheduling, so this needs to be above some minimum. This is measuring basically the parallelism per processor.

### Cilk Scheduler
It uses a work stealing scheduler. It achieves an expected time of T_1 / P + O(T_inf) which is T_1 / P + T_inf empirically. In that case
we can get near perfect linear speedup as long as the number of processors is low relative to the parallelism. Instrumentation of Cilk
almost always lets you measure T_1 and T_inf.

### Cilk Runtime System
On a high level each processor gets its own STACK. Each stack has either a call or a spawn. Each processor independently tries to consume its stack. If it finishes its stack before others it picks another random processor to steal from. From that processor it steals the TOPMOST elements from the last spawn up to the second to top spawn. Remember that because its a stack it's OK to do this. Normally spawns start up processes and "continue" them so it's OK to do this. (Remember that between two spawns in a strand.)

Note that processes are growing their stacks in parallel.

The idea is that if steals are sufficiently infrequent then we get a close to linear speedup.

#### Pseudo-Proof
Assuming uniformity and some other such things, because a processor is working or stealing at any point of time, it expects with probability 1 / P to take one away from the span. Thus it takes O(P x T_inf). Obviously, at least one of the processors is contributing to the span of the computation DAG. Since there are P processors, the time remains (T_1 + O(PT_inf)) / P = T_1 / P + O(T_inf) approximately. Basically, just think about how the steals get amortized.

#### Cactus Stack
C's rules for pointers: pointer to a stack space can be passed from parent to child but not vise versa. Thus, their views of the stack include only the path upwards. Cilk does this too, but it supports the views in parallel. They didn't explain this in much detail.

The stack space for a P-processor program is bounded by P x S_1 for S_1 being the stack necessary for the stack.

Busy leaves as proof for the above: each leaf of the computational graph has some processor processing on it (as long as it was invoked
by a parent). This means any stack that exists, is being used and therefore belongs to a processor. It's consumed upwards by that processor
later and then it will no longer exist. If it wasn't created then it's not taking space (and cannot be created without a processor being assigned to it).