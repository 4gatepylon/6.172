# Source - Assembly Language and Computer Architecture
https://www.youtube.com/watch?v=L1ung0wil9Y

# Notes
There are four stages to compilation: preprocessing, compiling, assembling and linking. You can use clang flags like -E, -S, -c, and lastly ld to preprocess, compile, assemble, and link respectively. Liniking is obviously for libraries.

Recall machine code and it's nearly 1:1 correspondence to assembly. We have tools (objdump) that we can use to turn machine code into assembly. This is helpful for seeing when optimizations happen, finding bugs, understanding what, say, Intel is doing in their fast libraries, etcetera.

They expect us to be able to read assembly (we use x86) with the aid of a manual/table and understand the effects of common patterns (etc).

## x86-64 Primer
ISA - Instruction Set Architecture

Intel vs At & T syntax. Recall registers, how we go linearly through memory unless there are jumps, etcetera. Legacy: there are registers which alias into other registers, but with smaller words (i.e. from when computers changed from 16 to 32 bit, there are 16-bit registers in the ISA that register to the corresponding 32 bit registers... I'm guessing this is for backwards compatibility).

Usually there are suffixes to tell you what the datatypes are (for example: b is a byte, w is a 2 byte (16 bit) word, l or d is a double word (32 bits) and q is a quad word (64 bits from when words were 16 bit); s is single precision float, d is double precision float, and t is extended precision (16(10) or somethig like that)).

Some commands, like mov, interact between 32 and 64 bits. movs for example will move (copy) and then keep the sign for the copied quantity. movzbl extends with zeros and the first operand is a byte and the second is a long (sometimes you'll have more than one suffix).

The 32 => 64 bit version zero-extends, while previously it had just ignored the high order bits. There are a bunch of annoying quirky things like this.

Conditional jumps (and conditionals) also use suffixes to tell you whether to jump (i.e. jne is jump if not equal).

You can jump on status flags and other things (these are in the RFLAGS register).

### Memory Addressing Modes
#### Direct
Immediate: give it a constant to store into a register (it's literally in the instruction).

Register: moves it (copies it) from the register.

Direct memory uses a specific memory location. NOTE: fetching something from memory (without caching) is around 200 cycles or more. Registers are single cycle.

#### Indirect
Register indirect: move the contents at the address in the first register into the second register.

Registered Index: same thing as register indirect, but add an offset.

Instruction pointer relevant: off instruction pointer register with an offset (so you can jump to where you are in the code plus some number of instructions or something else; sometimes you can do this with data in the instruction stream).

Most general form: base index scale displacement mode. It takes three operands and an offset. You have a base, then an index (think array) and then a scale (think size of array) and then that is led by an offset.

#### Jumps
They take labels (can be beginning of a function or something generated). They can also be exact addresses or indirect addresses.

#### Idioms
Sometimes you see things like xor %rax %rax and that zeroes out the register. This is quicker and easier than having a zero constant that is written to it.

Sometimes you see (note test is bitwise and) test %rax %rax and that is testing whether it's zero using the RFLAGS register.

x86 includes certain no-op operations "nop." There are idioms with no-ops. The compiler generates them I'm guessing to do alignment optimization for the code (start the next instruction at the beginning of a cache line) or injecting data (I'm guessing) or something like that.

**Check the flags and condition codes for review.**

## Floating Point and Vector Hardware
SSE opcodes have suffixes as well: single floating point, double floating point, vector of single or double floating points (etc). Check the table.

Floating point operations can be slower than integer ones I think.

### Vector Operations
Many versions supported: SSE (XMM 128 bit registers...), AVX, AVX2 (these next two are bigger), and AVX-512.

Sometimes vector operations need the values to be aligned in memory (by the vector length). Even when they support unaligned, sometimes they are slower.

Prefix v differentiates AVX from SSE (no v). The p prefix differentiates integer from floats. Similarly to the previous register aliasing thing I mentioned, the XMM registers alias the YMM registers.

## Overview of Computer Architecture
Recall the simple five steps of a processor: instruction fetch, instruction decode, execute, memory, write-back. It is worth revewing some 6.004 material here. This is pipelined to shorten the clock-cycle and improve throughput.

Intel/Haswell has a ton of different pipeline stages (14-19). It's crazy.

Usually people have made processor by making things parallel or close by. We've seen some parallelism with vector data, but there is also instruction level parallelism, multicore, etcetera.

Pipeline stalls: for correctness reasons you can't read the next instruction and need to wait for something in the pipeline to finish. Three types of hazards cause this: structural hazards (two instructions try to use the same functional unit at the same time), data hazard (when an instruction depends on the result of a prior result in the pipeline), control hazard (fetching and decoding the next instruction is delayed by a decision for control flow: the next instruction is not clear because we are waiting on certain data).

### Data Hazards
True dependence (first writes, second reads), anti-dependence (first one reads, second one writes), and output-dependence (first one writes then second writes: this is primarily for RFLAGS or aliasing I think).

### Operation Cycle Types
Most integer operations take one cycle, while floating point can take 3, 5, or more cycles (for add, mult and divide). Integer mult can take around 3 while division is variable. Floating point fused multiple add in their table takes 5 cycles.

The main takeaway is that the more you do the simple operations, the faster everything will be.

Hardware uses different hardware components/functional units to deal with these complicated operations (they may be pipeliend fully, partially, or not at all). With this seperation, computers can run floating point operations at the same time as other operations (you can do multiple instructions per cycle). This is called super-scalar processing (executing multiple things at a time).

### Tricks and whatnot
In 2018 they used Haswell intel computers. Those break up x86-64 instructions into simpler operations called micro-ops. The fetch and decode stages emit 4 micro-ops per cycle...

Bypassing: just take the output directly before writing and give it to the next instruction (i.e. read "true hazard" dependencies).

Scoreboarding, renaming, reordering...

### Branch Prediction
Mispredicted branch prediction can cost you (in their Haswell example it was 15-20 cycles).