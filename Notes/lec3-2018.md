# Source - Bit Hacks
https://www.youtube.com/watch?v=ZusiKXcz_ac

# Notes
Recall unsigned representation and twos complement (first bit is -2^largest power and the rest are positive 2^power). Note that x + ~x = -1 and x + -x = 0. Note that -x = ~x + 1.

Recall hex, the bitwise operators, etcetera.

Recall how to write 1 into a bit (or with shift) or clear (and with negation of shift) or toggle (xor with shift).

Recall how to set a mask (clear the bits by inverting the mask, then left shift the new mask value and or it).

## Swapping without a temporary variable
You can do this with bit tricks. `x = x ^ y`, `y = x ^ y`, `x = x ^ y`. This is so smart!

The disadvantage is that the instructions cannot be parallelized. This is a cool trick even if not that performant.

## Finding the minimum of two integers without branching
Without branching?

Note that C represents booleans `true` and `false` with the integers `1` and `0` respectively. There is an expression you can use: `r = y ^ ((x ^ y) & -(x < y))`. Say `x < y`, then this is `y ^ ((x ^ y) & -1) = y ^ (x ^ y) = x`. Say `y <= x`, then this is `y ^ ((x ^ y) & - (0)) = y ^ ((x ^ y) & 0) = y ^ 0 = y`.

## Branching Example
The `__restrict` keyword tells the compiler that there will only be one pointer to the given data and it will be that. Apparently that's good for optimizations.

Merging two sorted lists.

Normally you'd want to branches to be predictable (i.e. you can usually tell which branch path will be taken). All branches in the merge sort except the one that picks the larger element are predictable. How could we remove that branch? Brancheless min!

Store the compare value (one or zero) and then store the min (branchless) then add comp to the one that we were checking minimum for and add the negation to the other one. Add the minimum to the merged array (check the video to see what I mean).

**It is possible to do merge sort where the merge has no if statement (only the whiles)**. HOWEVER, the compiler usually optimizes the code better than we do, so it doesn't work... It is important, though, to undestand these tricks because you can read the assembly better. Also, you can extend thme to vectors. Also, sometimes you will use them (rarely).

## Bit Trick That Does Work
Say you want to do `x + y % n` where `0 <= x < n` and `0 <= y < n`. Note that modulus expression does division, which is usually expensive unless it's a power of two. You could do it with branching, but it's unpredictable. Instead, then you can use the minimum trick by letting `z = x + y` and then `z = z - ?` where the question mark is `n` if `z >= n` for which we use our min trick basically. Can you do it? `n & -(z >= n)`. The negative sign is important!

## Rounding a number to the nearest (higher) power of two
First decrement to set the right bits to one and if it is a power of two elimnate that top bit: `n--`, then populate (exponentially with a fold-like operation) all the bits to the right of any bit up to any length by `n |= n >> 1`, `n |= n >> 2`, `n |= n >> 4`, `n |= n >> 8`, `n |= n >> 16`, and `n |= n >> 32`. Now the number is filled with ones, so add one to get that higher power of two: `n++`.

## Least significant one (not necessarily 1's place, could be some other)
i.e. for `12 = 000...01100` the least significant one's mask would be `000... 00100`. You can do it with `x & (-x)`. The negative negates and then deletes the rightmost ones (which were the zeros in the actual number). Then when you and the other values are negated except the least significant one, which got incremented by the `-x = ~x + 1` propagation from zero (flipped) into one.

**How could you find the index?** i.e. the log base 2 of a power of 2. Remember the de-bruijn meme.

The De-Bruijn sequence is a cyclic sequence of 0's and 1's of length `2^k` such that each of the `2^k` `0-1` strings of length `k` occurs exactly once (there are `2^k` obviously because each bit can be zero or one). Then we store in an array the mapping (from index 0, 1, ... representing the numerical value of the k-bit sequence) the k-bit number TO the offset (shift-wise) to the RIGHT of the place it might occur (since these are obviously not necessarily in order).

Then we can multiply this DB sequence by the power of two (equivalent by the power's bit shifts to the left). Then the number will be furthest to the left, so we shift it down by `2^k - k`. It's important to have the sequence be led by zeros so that when we do this operation, if it shifts left very far, eating up some of the number, we still know what was at the beginning easily: otherwise we may need hella branching I think).

IMPORTANT: review this slide to get the ideas. How could this be expanded elsewhere?

I did NOT totally understand the card trick. How did they do the magic trick? They had 32 cards, each card is a 5-bit number.

There is a hardware instruction for this bit-trick and luckily we don't need to implement it yet.

## N-Queens problem
Beautiful representation: one bit-vector of length `n`, one of length `2n - 1` and another of `2n -1`. We have a 1 in each of these IFF there is a queen on that column or diagonal left or diagonal right respectively. Because we recurse on the rows, we do not need a bit-vectors to know whether there is a queen on that row. Therefore, it's an O(1) operation with a bit shift and and to check if a queen is at that location. Updating is also easy.

**How else could we use this idea?**

## Population Count: Count the number of 1 bits in a word x
Naive: use for loop for each bit.

Better: clear lest significant one bit in x (`x & (x - 1)`) and then count number of iterations to reach zero. This will have one iteration bet one.

Yet Better: table lookup. Create a table of size `2^k` for the `k` bit words where each index is the k-bit word. Then simply look it up. For longer numbers, store, for example, that for each eight bit word, then look up for the last eight bits, shift, then for the next eight bits, shift, etc... while it's larger than zero. However, this approach is bottleknecked by lookup.

NOTE:
- Register lookup is around 1 cycle
- L1 cache is around 4 cycles
- L2 cache is around 10 cycles
- L3 cache is around 50 cycles
- DRAM is around 150 cycles

**Only Register Approach**: they define five hellish masks and do a bunch of bit operations. REVIEW this if you need it. The idea is for each pair of bits to tell how many ones there are in that group. Then merge every pair of pairs, then every pair of pairs, and so on.

Basically for the register only approach, you start with 01010101010101... and you and that and then that shifted by one to get that pair. Then you do 001100110011 and that shifted by two to get every pair, then so on (00001111 and merging up).

Most modern machines have popcount in hardware and you can access it with gcc or clang i.e. `__builtin__popcount`.

How could you compute the log base two of a number using popcount quickly? How about a power of two. You could subtract one from the power of two. For the other number you could use a previous trick and then apply popcount with the subtract. Alternatively, you could calculate something like `branchless_max((left_offset + 32) * -(pop_count_mask_left >= 1), right_offset * -(pop_count_mask_right >= 1)` where the brancless max is just branchless min inverted (i.e. XOR 1) and offset right or left is this expression, except applied next with 16, 8, 4, 2, 1 and the pop count mask is just the correspondingly sized mask from the popcount algorithm. USING NO POPCOUNT!