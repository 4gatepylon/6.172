# Source - What Compilers Can and Cannot Do
https://www.youtube.com/watch?v=ulJm7_aTiQM

# Notes
Once the code is compiled to LLVMIR usually there are multiple passes of different transforms (which may be repeated)
which the compilers put in a heuristically good order. These transforms change the code in certain was to optimize
the code without changing its functionality hopefully.

Clang LLVM allows you to request reports for multiple types of optimization (not just vectorization). You can use the
flags -Rpass=something to get a report for "somethings" that were successful (i.e. a type of optimization). You can also
add -Rpass-missed to say which are missed and -Rpass-analysis to analyse those optimizations. The argument "something" is
a regex (so when in doubt just use .*).

Reports are useful, but long and hard to understand sometimes.

Most optimizations it does are akin to the bentley rules with some extras. It does a lot of smart stuff to try and
use registers as much as possible, try to be good for the architecture, etcetera.

Recall the leal operation: load effective address.

Recall eax/rax aliasing. Also note how you can do multiply by 3 by adding 2 times itself to it (and so on for any one of these numbers).

## Optimizing a scalars and Structures
The compiler is by default very stupid. Everything it is given, it tends to allocate local storage, store to it, and then fetch it when needed. Much optimization just involves not allocating when necessary. It seems like in LLVMIR because there are infinite registers it just
is able to put a lot of it into registers. It's main heuristic for scalars and structs is to do as much WITHOUT memory as possible.

Note that for structs it just optimizes one field at a time. Remember the example of the vector scaling code. It's honestly a little
wierd to me how it starts out so much less like the C code than it ends up.

## Optimizing a Function Call
Basically the majority of what it does it removing useless operations (remember for structs, like how it removed pack unpacks, etcetera).

It does a lot of inlining.

Obviously recursive functions can't really be inlined (except for tail recursion), and also sometimes functions in other compilation units
(i.e. in other files). Sometimes inlining will hurt performance because the code will get too long (and maybe miss cache or whatever). The
compiler has a cost model that gives it information about what's probably going to make the code faster or slower. It makes a best guess
using its cost model, but it doesn't really know (sometimes it guesses wrong).

Suggestions
- Use `__attribute__((always_inline))` and `__attribute__((no_inline))` to tell it what should always be inlining or not
- Use link-time optimization (LTO) to enable program-wide optimization (and thus cross-file inlining)

The inline keyword only gives it a hint.

## Optimizing Loops
A big part of compiler optimization is on loops becuase they are so common and account for a lot of the execution time of programs.

Code Hoisting (aka loop-invariant code motion: LICM). Hoisting: when you have, say, nested loops, and the things depend 
only on the outer loop, "hoist" them out of the loop in the inner loop. This way you can save some work.

Loops can be vectorized. Even if it's unclear if the arrays overlap in their example, it's smart enough to make a conditional that chooses.

Compilers tend to be very conservative with pointer aliasing (i.e. for arrays). You can annotate instructions with various metadata like
"restrict" which turns into tags like `!noalias` and whatnot. Always annotate your pointers!

## Diagnosing Failures (Case Studies)
?