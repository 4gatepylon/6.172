# Source - Bentley Rules for Optimizing Work
https://www.youtube.com/watch?v=H-1-X9bkop8

# Notes
The work of a program is the sum total of the number of operations the program needs to execute. The reduction of work does not necessarily speed it up (i.e. with algorithm design) even though it does in theory. Remember: branch prediction, parallelism, caches, etcetera.

"Bentley Rules" are a set of heuristics (originally from a book) that we've extracted and reworked to be architecture-independent and help us figure out how we might do less work.

Here is the list:
## Data Structures
- Packing and encoding: the idea of packing is to encode more than one data value in a machine word and the idea of encoding is to use a representation that takes fewer bits. This is good because we might use less memory fetches (a lot of time is spent moving things around in memory). Example: dates. C has bitfields and you can tell it how many bits each int needs in a struct. Soemtimes you might want to unpack values if the work is in the representation's related work instead of memory movement.
- (Data Structure) Augmentation: add information to make common operations do less work. (Recall the binary tree examples.) Their example is the linked-list implementation. If you want to be able to append a list, then maybe keep a pointer to the last element.
- Precomputation: perform calculations in advance to avoid doing them in mission-critical times. Example: using binomial coefficients. You may reuse these a lot and in those cases it's useful. We can use Pascal's triangle to make computation of future coefficients very fast (just add two numbers).
- Copmile-time initialization: basically fo the binomial coefficient example put that table in the code. What if you wanted a 1000x1000 table though? Program to write your program duh.
- Caching: store results that will be accessed again a lot or have been recently used. Example: static variable or set.
- Lazy evaluation: he didn't talk about this, but the idea is to only do work once it is really needed (not ALL work might be needed).
- Sparsity: avoid computing on "zero" elements. Example: *compressed sparse row* for matrix-vector multiplcation. Rows array stores the offset into the cols array. The Cols array stores the location (i.e. which col in the row that was corresponding) and then there is a value array which has the value at that location. this seems not bad for caching compared to naive sparisty. Also recall the sparse graph example (those second lists, can be one long list with offsets). Recall storing weights in the same array as the edge: cache locality.

## Loops
- Hoisting: don't recompute loop-invariants every iteration. He gives an example of adding the same constant to every element in an array.
- Sentinels: special dummy values you insert in, say, an array to simplify the handling of loop exit tests or other boundaries! I did not think of this! Overflow example with the max int appended to the array. Basically, if you overflow early you know that the array overflows, while otherwise you'd need to check every iteration or at the end (slower). He adds the one at the end to deal with the boundary condition where all the elements are zero.
- Loop unrolling: full and partial. The idea is to unroll some iterations to do less iterations, each of which may be able to combine some work. Full loop unrolling is uncommon. Loop unrolling if there are a lot of instructions can be bad because it'll pollute your instruction cache! He gives an example of partial loop unrolling. The first benefit is that there are less checks. Also, it gives the compiler more freedom to optimize the loop body.
- Loop fusion: combine multiple loops in the same index range into the same loop to save overhead from loop control. He gives the min/max example. In the min/max example it's also better due to cache locality.
- Eliminating wasted iterations: obvious... (n^2 transpost vs nC2 transport example).

## Logic
- Constant folding and propagation: evaluate all constants at compilation time. Recall example of orrery (solar system model) given a radius.
- Common-subexpression elimination: evaluate a thing once and then store for later use.
- Algebraic identities: example of two balls with radius and using squared distance instead of distance.
- Short-circuiting: stop evaluating series of tests once we know the answer, maybe use the shortest (or expected to be shortest) path to find out early rather than layer. The example with the if statement inside the loop (note that it's not always faster obviously). Also && vs. & and | vs. ||.
- Ordering tests: as with short circuiting, order the tests so that the more inexpensive or more successful ones happen first and the others later. Example with whitespace (check whitespace instead of carriage return first).
- Creating a fast path: example with graphics, bounding boxes for balls (maybe that's even parallelizeable). Create a quick test that you run before the expensive test to get rid of a lot of false negatives (or positives, or whatever).
- Combining tests: example with the switch statement.

## Functions
- Inlining: avoid overhead to function call (i.e. stack) by replacing calls to the function by the body itself. If you want to inline a function you can declare it as "static inline". They are usually just as efficient as macroes, though the macroes aren't smart enough to do common subexpression elimination and other such things.
- Tail-recursion elimination: when you have a loop implemented recursively, just turn it into an actual loop? They don't go over it in the lecture.
- Coarsening recursion: basically make the base case larger.

## Closing Remarks
Avoid premature optimization: make sure your code is functional first by creating regression tests. Remember, reducing work does not ALWAYS reduce running time, but it is a good heuristic. Assembly code output can tell us what the compiler is optimizing.