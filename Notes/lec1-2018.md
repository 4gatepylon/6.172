# Source - Introduction and Matrix Multiplication
https://www.youtube.com/watch?v=o7h_sYMk_oc

# Notes
## Intro
Performance is not the most important goal from the point of view of progammers (usually it's correctness or other such properties). He likes to think of performance as a budget and you sacrifice your performance to buy other nice properties. Usually, users care a lot more about performance than we do as programmers.

Some history: Moore's law a Dennard Scaling (as transistors get smaller their power density stays constant, so that the power use stays in proportion to the area, meaning that the power is the same, but the paths are shorter, making the clock frequency faster). Lots of optimization in the 60's before this point. Little between then and now. At this point power density is growing again because now the power is not dynamic power (when you turn on the machine) and more of it is static power (leakage). That would heat up the junctions too much.

When more's law stopped, they started adding more cores (around 2005 this started) and using parallelism. Similarly: vector units, steeper cache heirarchies, GPUs, FPGAs, and other such optimizations. In these cases the software has to be adapted to deal with performance on these variegated hardware platforms.

Modern processors are very complicated and how do we write software for that?

## Matrix Multiplication Example
Seems like they give us a rundown of the machine we are using and its performance characteristics (i.e. in 2018 they used AWS c4.8xlarge). There are things like hyperthreading (intel's proprietary multithreading), fused multiply-add for floating point numbers, some number of caches and processors, etcetera.

### Python Performance Example
#### Cubic Nested Loop
Around 6 hours for a 4096 x 4096 matrix. That's around 6.25 megaflops where the machine had at least 800 megaflops. So clearly not fast (look at the lecture).

### Java Performance Example
#### Cubic Nested Loop
Around 46 minutes (9x speedup).

### C Performance Example
#### Cubic Nested Loop
Clang LLVM 6.0 compiler. Around 19 minutes (2x java, 18x Python). Obviously Python was much, much slower because Python is interpreted.

#### Changing the Order Of The Loops
Why? Cache locality! It can affect the runtime by a factor of 18! We want consecutive operations to be all using data from the cache. The matrices are in row-major order.

There are tools we can help us to figure this out (i.e. to figure out the miss rates for cache). For example: valgrind's cachegrind.

Relative speedup overall is 6.5x.

#### Changing the optimization level (flag) in the compiler
They set it to O2 (sometimes it does beat O3 since these are heuristics). There are also other tools to use the runtime results to optimize more smartly (etc). This is a factor of 3.25x speedup.

#### Parallelizing with cilk
You use cilk_for instead of for (loop). It seems to handle a lot of the parallelization for you. You can't do it for all loops (why: I think it's because you'd combine the wrong A and B elements since the memory is shared, but I'm not totally sure; it depends on how cilk does it's thing I think).

It was better to parallelize just i than i and j. Why? This has to do with scheduling overhead. This is an almost 18x speedup (luckily it was easy to parallelize). Around 5% of peak!

#### Restructuring to reuse data in the cache the most possible
*Blocks* instead of rows! This scheme is called "tiling." You use two tiers of loops: an outer to do the normal thing on the blocks, and then an inner to do the loops. The tile size is big enough to matter and small enough to fit in the cache. The tile size is a tuning parameter. We find out which one is good by testing them (who knows what else is going on in the cache: you also might want to have both inputs in the cache or something else like that). This tiled approach reduces cache misses by up to 68%.

This is complicated since there are many levels of caching (i.e. L1 data and instruction, L2 for processor and L3 shared, then memory). Thus, they can search for two-layer tiling (i.e. each tile has tiles inside it).

To make it a little better they use recursion to cut it into quarters. The base case uses the old simple algorithm on some block-size. "Parallel divide and conquer." This is now 12% of theoretical maximum.

#### Vectorising with the Compiler
Clang vectorizes naturally, but you can get a report sometimes with Rpass=vector to know what it's doing. You can use flags to tell it what to vectorize (or what architecture you're running on, etc...).

They then also use -ffast-math to tell the compiler that associativity does not matter for double precision floats (basically, for floats because of round-off a*b*c != a*c*b and so on, and the compiler thinks you care about the order, but if you don't, it can optimize potentially better).

### Other performance insights
- Preprocessing
- Matrix transposition (for this)
- Data alignment
- Clever algorithms for the base case
- Memory management

## Notes About The Class
6.172 will be about mastering multi-core performance engineering. We do not deal with filesystem or network or gpus or some other form of optimization. Those are also hugely important but he things that multi-core is often the most important one to start with since it gives you a good understanding and skillset to grow later.